<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="keywords" content="mechatronic engineering &amp; electronic engineering"><meta name="description" content="Hexo Theme Redefine"><meta name="author" content="HuangYaohui"><title>Pytorch Build The Neural Network | HuangYaohui</title><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://evan.beee.top" crossorigin><link rel="stylesheet" href="/css/style.css"><link rel="shortcut icon" href="/images/logo.svg"><link rel="stylesheet" href="/css/fontawesome.min.css"><link rel="stylesheet" href="/css/brands.min.css"><link rel="stylesheet" href="/css/solid.min.css"><link rel="stylesheet" href="/css/regular.min.css"><link rel="stylesheet" href="/css/css2.css"><script id="hexo-configurations">let REDEFINE=window.REDEFINE||{};REDEFINE.hexo_config={hostname:"username0occupied.github.io",root:"/",language:"en",path:"search.xml"},REDEFINE.theme_config={toc:{enable:!0,number:!1,expand_all:!0,init_open:!0},style:{primary_color:"#005080",avatar:"/images/HeadImage.jpg",favicon:"/images/logo.svg",article_img_align:"center",right_side_width:"210px",content_max_width:"1000px",nav_color:{left:"#f78736",right:"#367df7",transparency:35},hover:{shadow:!0,scale:!1},first_screen:{enable:!0,background_image:{light:"/images/PostImage.jpg",dark:"/images/DarkBackground.jpg"},title_color:{light:"#fff",dark:"#d1d1b6"},description:null},scroll:{progress_bar:{enable:!0},percent:{enable:!1}}},local_search:{enable:!0,preload:!0},code_block:{copy:!0,style:"mac"},pjax:{enable:!0},lazyload:{enable:!0},version:"1.1.0",friend_links:{columns:2}},REDEFINE.language_ago={second:"%s seconds ago",minute:"%s minutes ago",hour:"%s hours ago",day:"%s days ago",week:"%s weeks ago",month:"%s months ago",year:"%s years ago"}</script><meta name="generator" content="Hexo 6.3.0"></head><body><div class="progress-bar-container"><span class="scroll-progress-bar"></span> <span class="pjax-progress-bar"></span> <span class="pjax-progress-icon"><i class="fa-solid fa-circle-notch fa-spin"></i></span></div><main class="page-container"><div class="page-main-content"><div class="page-main-content-top"><header class="header-wrapper"><div class="header-content"><div class="left"><a class="logo-title" href="/">HuangYaohui</a></div><div class="right"><div class="pc"><ul class="menu-list"><li class="menu-item"><a href="/"><i class="fa-regular fa-house"></i> HOME</a></li><li class="menu-item"><a href="/archives"><i class="fa-regular fa-archive"></i> ARCHIVES</a></li><li class="menu-item"><a href="/categories"><i class="fa-solid fa-indent"></i> CATEGORIES</a></li><li class="menu-item"><a href="/about"><i class="fa-regular fa-user"></i> ABOUT</a></li><li class="menu-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i></li></ul></div><div class="mobile"><div class="icon-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i></div><div class="icon-item menu-bar"><div class="menu-bar-middle"></div></div></div></div></div><div class="header-drawer"><ul class="drawer-menu-list"><li class="drawer-menu-item flex-center"><a href="/"><i class="fa-regular fa-house"></i> HOME</a></li><li class="drawer-menu-item flex-center"><a href="/archives"><i class="fa-regular fa-archive"></i> ARCHIVES</a></li><li class="drawer-menu-item flex-center"><a href="/categories"><i class="fa-solid fa-indent"></i> CATEGORIES</a></li><li class="drawer-menu-item flex-center"><a href="/about"><i class="fa-regular fa-user"></i> ABOUT</a></li></ul></div><div class="window-mask"></div></header></div><div class="page-main-content-middle"><div class="main-content"><div class="fade-in-down-animation"><div class="post-page-container"><div class="article-content-container"><div class="article-title"><span class="title-hover-animation"><h1 style="font-size:2rem;font-weight:700;margin:10px 0">Pytorch Build The Neural Network</h1></span></div><div class="article-header"><div class="avatar"><img src="/images/HeadImage.jpg"></div><div class="info"><div class="author"><span class="name">HuangYaohui</span></div><div class="meta-info"><div class="article-meta-info"><span class="article-date article-meta-item"><i class="fa-regular fa-pen-fancy"></i>&nbsp; <span class="pc">2023-02-04 20:22:48</span> <span class="mobile">2023-02-04 20:22</span> </span><span class="article-categories article-meta-item"><i class="fa-regular fa-folders"></i>&nbsp;<ul><li><a href="/categories/python/">python</a>&nbsp;</li><li>&gt; <a href="/categories/python/pytorch/">pytorch</a>&nbsp;</li></ul></span><span class="article-tags article-meta-item"><i class="fa-regular fa-tags"></i>&nbsp;<ul><li><a href="/tags/python/">python</a>&nbsp;</li></ul></span><span class="article-wordcount article-meta-item"><i class="fa-regular fa-file-word"></i>&nbsp;<span>982 Words</span> </span><span class="article-min2read article-meta-item"><i class="fa-regular fa-clock"></i>&nbsp;<span>4 Mins</span> </span><span class="article-pv article-meta-item"><i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span></span></div></div></div></div><div class="article-content markdown-body"><h1 id="建立神经网络"><a href="#建立神经网络" class="headerlink" title="建立神经网络"></a>建立神经网络</h1><p>看不懂的先跳过。</p><p>神经网络由数据执行操作的层&#x2F;模块组成。torch.nn命名空间提供了你构建神经网络的所有构建块。一个神经网络自身就包含了其他模块。</p><p>下面将建立一个神经网络对FashionMNIST数据集进行分类</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br></pre></td></tr></table></figure></div><p>尝试使用cuda</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Using <span class="subst">&#123;device&#125;</span> device&quot;</span>)</span><br></pre></td></tr></table></figure></div><p>通过继承nn.Module来创建神经网络的子类（一个模型），每个nn.Module子类都通过forward（前向传播）方法输入数据</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(NeuralNetwork, self).__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure></div><blockquote><p>flatten方法的作用：实现简单的数据降维，把某些维度的数据重新编排成一个维度。 torch.nn.Flatten(<em>start_dim&#x3D;1</em>, <em>end_dim&#x3D;- 1</em>)，默认把除了第0维的所有数据重拍成一个维度</p><p>Sequential的作用：它是一个容器允许模型顺序传入，顺序字典类型也能传入，把不同模块联系起来形成训练网络</p><p>Linear的作用：建立一个线性函数$$y&#x3D;xA^T+b$$</p><p>ReLu:参数*u()，保证非负</p><p><a class="link" target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten">https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p><a class="link" target="_blank" rel="noopener" href="https://www.jianshu.com/p/9b6bde60f3e9">https://www.jianshu.com/p/9b6bde60f3e9 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p><a class="link" target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential">https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p><a class="link" target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear">https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p></blockquote><p>建立一个NeuralNetwork实例，把他放入设备中并打印结构</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = NeuralNetwork().to(device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure></div><p>使用这个模型，我们需要往里面传入数据，应该使用模型的forward方法？？</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, device=device)</span><br><span class="line">logits = model(X)</span><br><span class="line">pred_probab = nn.Softmax(dim=<span class="number">1</span>)(logits)</span><br><span class="line">y_pred = pred_probab.argmax(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted class: <span class="subst">&#123;y_pred&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></div><p>调用模型将会返回一个二维的Tensor，其中最后的输出是每个类型是10个原始预测输出，使用nn.Softmax进行归一化，得到最终概率</p><h1 id="代码详解"><a href="#代码详解" class="headerlink" title="代码详解"></a>代码详解</h1><h2 id="建立图像"><a href="#建立图像" class="headerlink" title="建立图像"></a>建立图像</h2><p>模拟建立三幅28*28的灰度图像</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input_image = torch.rand(<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(input_image.size())</span><br></pre></td></tr></table></figure></div><h2 id="数据降维"><a href="#数据降维" class="headerlink" title="数据降维"></a>数据降维</h2><p>把一副图像的二维数据转换成一维</p><p>flatten具体内容上方已提及</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">flatten = nn.Flatten()</span><br><span class="line">flat_image = flatten(input_image)</span><br><span class="line"><span class="built_in">print</span>(flat_image.size())</span><br></pre></td></tr></table></figure></div><h2 id="线性函数"><a href="#线性函数" class="headerlink" title="线性函数"></a>线性函数</h2><p>$$<br>y&#x3D;xA^T+b<br>$$</p><p>torch.nn.Linear(<em>in_features</em>, <em>out_features</em>, <em>bias&#x3D;True</em>, <em>device&#x3D;None</em>, <em>dtype&#x3D;None</em>)</p><p>下面将会看到线性函数中A的样子</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">layer1 = nn.Linear(in_features=<span class="number">28</span>*<span class="number">28</span>, out_features=<span class="number">20</span>)</span><br><span class="line">hidden1 = layer1(flat_image)</span><br><span class="line"><span class="built_in">print</span>(hidden1.size())</span><br></pre></td></tr></table></figure></div><h2 id="非线性化"><a href="#非线性化" class="headerlink" title="非线性化"></a>非线性化</h2><p>把小于0的数变成0，大于等于0的数保持原样</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Before ReLU: <span class="subst">&#123;hidden1&#125;</span>\n\n&quot;</span>)</span><br><span class="line">hidden1 = nn.ReLU()(hidden1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;After ReLU: <span class="subst">&#123;hidden1&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></div><h2 id="容器模块"><a href="#容器模块" class="headerlink" title="容器模块"></a>容器模块</h2><p>nn.Sequential可以把输入的模块按顺序的排列在容器中，形成新的模块（神经网络），传入容器的数据会被按顺序执行，这里对数据降维，线性映射，非线性化，再线性映射</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">seq_modules = nn.Sequential(</span><br><span class="line">    flatten,</span><br><span class="line">    layer1,</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">20</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line">input_image = torch.rand(<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">logits = seq_modules(input_image)</span><br></pre></td></tr></table></figure></div><h2 id="概率归一化"><a href="#概率归一化" class="headerlink" title="概率归一化"></a>概率归一化</h2><p>把一个n维数据进行尺度变换，使得每一个数都落在[0,1]，所有数据加和为1<br>$$<br>Softmax(x_i)&#x3D;\frac{\exp(x_i)}{\sum_j \exp(x_j)}<br>$$<br>神经网络的最后一层是logits，使用nn.Softmax模块转换为每个类别的概率值</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">softmax = nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">pred_probab = softmax(logits)</span><br></pre></td></tr></table></figure></div><h2 id="模型参数"><a href="#模型参数" class="headerlink" title="模型参数"></a>模型参数</h2><p>一个神经网络里的层都是参数化的，例如权重和偏置，nn.Module会自动记录所有变量，可以使用parameters()或named_parameters()方法获得参数。</p><p>在这里我们可以看到神经网络的参数</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model structure: <span class="subst">&#123;model&#125;</span>\n\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Layer: <span class="subst">&#123;name&#125;</span> | Size: <span class="subst">&#123;param.size()&#125;</span> | Values : <span class="subst">&#123;param[:<span class="number">2</span>]&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure></div><blockquote><p>参考：</p><p><a class="link" target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html">https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p><a class="link" target="_blank" rel="noopener" href="https://blog.csdn.net/qq_40379132/article/details/124547449">https://blog.csdn.net/qq_40379132/article/details/124547449 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p></blockquote></div><div class="post-copyright-info"><div class="article-copyright-info-container"><ul><li>Post title：Pytorch Build The Neural Network</li><li>Post author：HuangYaohui</li><li>Create time：2023-02-04 20:22:48</li><li>Post link：https://username0occupied.github.io/2023/02/04/Pytorch-Build-The-Neural-Network/</li><li>Copyright Notice：All articles in this blog are licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> unless stating additionally.</li></ul></div></div><ul class="post-tags-box"><li class="tag-item"><a href="/tags/python/">#python</a>&nbsp;</li></ul><div class="article-nav"><div class="article-next"><a class="next" rel="next" href="/2023/02/03/Pytorch-Transforms/"><span class="title flex-center"><span class="post-nav-title-item">Pytorch Transforms</span> <span class="post-nav-item">Next posts</span> </span><span class="right arrow-icon flex-center"><i class="fa-solid fa-chevron-right"></i></span></a></div></div><div class="comment-container"><div class="comments-container"><div id="comment-anchor"></div><div class="comment-area-title"><i class="fa-solid fa-comments">&nbsp;Comments</i></div><div id="waline"></div><script data-pjax src="//evan.beee.top/js/waline.js"></script><script data-pjax>function loadWaline(){Waline.init({el:"#waline",serverURL:"https://comment.xhyz.top/",lang:"zh-CN",dark:'body[class~="dark-mode"]',requiredMeta:["nick","mail"]})}{const e=setTimeout(()=>{loadWaline(),clearTimeout(e)},1e3)}</script></div></div></div><div class="toc-content-container"><div class="post-toc-wrap"><div class="post-toc"><div style="font-size:1.3rem;margin-top:0;margin-bottom:.8rem;transition-duration:.1s"><i class="fa-solid fa-list"></i> <strong>Contents</strong></div><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BB%BA%E7%AB%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-text">建立神经网络</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E8%AF%A6%E8%A7%A3"><span class="nav-text">代码详解</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BB%BA%E7%AB%8B%E5%9B%BE%E5%83%8F"><span class="nav-text">建立图像</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%99%8D%E7%BB%B4"><span class="nav-text">数据降维</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0"><span class="nav-text">线性函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%8C%96"><span class="nav-text">非线性化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%B9%E5%99%A8%E6%A8%A1%E5%9D%97"><span class="nav-text">容器模块</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-text">概率归一化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="nav-text">模型参数</span></a></li></ol></li></ol></div></div></div></div></div></div></div><div class="page-main-content-bottom"><footer class="footer"><div class="info-container"><div class="copyright-info info-item">&copy; <span>2022</span> - 2023&nbsp;<i class="fa-solid fa-heart icon-animate"></i>&nbsp;<a href="/">HuangYaohui. All Rights Reserved.</a></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="website-count info-item"></div><div class="theme-info info-item">Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v1.1.0</a></div><script async data-pjax defer>function odometer_init(){let e=document.getElementsByClassName("odometer");for(i=0;i<e.length;i++)od=new Odometer({el:e[i],format:"( ddd).dd",duration:200})}odometer_init()</script><div id="start_time_div" style="display:none">2022/12/30 23:59:59</div><div>Blog up for <span class="odometer" id="runtime_days"></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec</div></div></footer></div></div><div class="post-tools"><div class="post-tools-container"><ul class="tools-list"><li class="tools-item page-aside-toggle"><i class="fa-regular fa-outdent"></i></li><li class="go-comment"><i class="fa-regular fa-comments"></i></li></ul></div></div><div class="right-bottom-side-tools"><div class="side-tools-container"><ul class="side-tools-list"><li class="tools-item tool-font-adjust-plus flex-center"><i class="fa-solid fa-magnifying-glass-plus"></i></li><li class="tools-item tool-font-adjust-minus flex-center"><i class="fa-solid fa-magnifying-glass-minus"></i></li><li class="tools-item tool-expand-width flex-center"><i class="fa-solid fa-left-right"></i></li><li class="tools-item tool-dark-light-toggle flex-center"><i class="fa-solid fa-moon"></i></li><li class="tools-item tool-scroll-to-top flex-center"><i class="fa-solid fa-arrow-up"></i></li><li class="tools-item tool-scroll-to-bottom flex-center"><i class="fa-solid fa-arrow-down"></i></li></ul><ul class="exposed-tools-list"><li class="tools-item tool-toggle-show flex-center"><i class="fa-solid fa-cog fa-spin"></i></li></ul></div></div><div class="image-viewer-container"><img src=""></div><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-input-field-pre"><i class="fa-solid fa-keyboard"></i></span><div class="search-input-container"><input autocomplete="off" autocorrect="off" autocapitalize="off" placeholder="Search..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa-solid fa-times"></i></span></div><div id="search-result"><div id="no-result"><i class="fa-solid fa-spinner fa-spin-pulse fa-5x fa-fw"></i></div></div></div></div></main><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/header-shrink.js"></script><script src="/js/back2top.js"></script><script src="/js/dark-light-toggle.js"></script><script src="/js/local-search.js"></script><script src="/js/code-copy.js"></script><script src="/js/lazyload.js"></script><script src="/js/runtime.js"></script><script src="/js/odometer.min.js"></script><link rel="stylesheet" href="/css/odometer-theme-minimal.css"><div class="post-scripts pjax"><script src="/js/toc-toggle.js"></script><script src="/js/libs/anime.min.js"></script><script src="/js/toc.js"></script></div><script src="/js/libs/pjax.min.js"></script><script>window.addEventListener("DOMContentLoaded",()=>{window.pjax=new Pjax({selectors:["head title",".page-container",".pjax"],history:!0,debug:!1,cacheBust:!1,timeout:0,analytics:!1,currentUrlFullReload:!1,scrollRestoration:!1}),document.addEventListener("pjax:send",()=>{REDEFINE.utils.pjaxProgressBarStart()}),document.addEventListener("pjax:complete",()=>{REDEFINE.utils.pjaxProgressBarEnd(),window.pjax.executeScripts(document.querySelectorAll("script[data-pjax], .pjax script")),REDEFINE.refresh()})})</script></body></html>